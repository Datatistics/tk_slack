{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a8fdd13",
   "metadata": {},
   "source": [
    "# snowflake_connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e604e305",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp snowflake_connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5d9c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08569f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastcore.basics import patch_to\n",
    "from fastcore.test import *\n",
    "\n",
    "from typing import List, Tuple, Dict, Any, Callable, Optional\n",
    "\n",
    "import snowflake.connector\n",
    "from snowflake.connector.pandas_tools import write_pandas\n",
    "\n",
    "import pandas as pd\n",
    "import json, re, os\n",
    "from datetime import datetime\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cb13c7",
   "metadata": {},
   "source": [
    "# SnowflakeConnector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c176fb08",
   "metadata": {},
   "source": [
    "Connector class for Snowflake operations related to Slack interactions. Handles connecting to Snowflake and provides methods for data operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8d196f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class SnowflakeConnector:\n",
    "    \"\"\"\n",
    "    Connector class for Snowflake operations related to Slack interactions.\n",
    "    Handles connecting to Snowflake and provides methods for data operations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, connection_params: Optional[Dict[str, Any]] = None):\n",
    "        \"\"\"Initialize the Snowflake connector.\n",
    "        \n",
    "        Args:\n",
    "            connection_params: Dictionary with connection parameters.\n",
    "                If None, will use environment variables.\n",
    "        \"\"\"\n",
    "        # Use provided params or get from environment\n",
    "        if connection_params:\n",
    "            self.connection_params = connection_params\n",
    "        else:\n",
    "            self.connection_params = {\n",
    "                'account': os.environ.get('SNOWFLAKE_ACCOUNT'),\n",
    "                'user': os.environ.get('SNOWFLAKE_USER'),\n",
    "                'password': os.environ.get('SNOWFLAKE_PASSWORD'),\n",
    "                'warehouse': os.environ.get('SNOWFLAKE_WAREHOUSE'),\n",
    "                'database': os.environ.get('SNOWFLAKE_DATABASE'),\n",
    "                'schema': os.environ.get('SNOWFLAKE_SCHEMA')\n",
    "            }\n",
    "        \n",
    "        # Connection is initialized lazily when needed\n",
    "        self._conn = None\n",
    "        self._schema_cache = {}\n",
    "        self.database = self.connection_params['database']\n",
    "        self.schema = self.connection_params['schema']  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c588f6ce",
   "metadata": {},
   "source": [
    "First, if we want to make a Snowfalke Connection class, we should probably make a way to connect to Snowflake..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6e2e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@patch_to(SnowflakeConnector, cls_method=True)\n",
    "def _get_connection(self):\n",
    "    \"\"\"Get a Snowflake connection, creating one if needed.\"\"\"\n",
    "    if not hasattr(self, '_conn'):\n",
    "        self._conn = snowflake.connector.connect(\n",
    "            **self.connection_params\n",
    "        )\n",
    "    return self._conn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd52b52",
   "metadata": {},
   "source": [
    "Yay! We are now able to connect to Snowflake which is pretty cool. Let's also make a method to close the connection when we are done with it. This is important because we don't want to leave connections open and use up resources. We can do this by adding a `close` method to our class. This method will call the `close` method on the connection object. We can also add a check to see if the connection is already closed before trying to close it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0e9aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch_to(SnowflakeConnector, cls_method=True)\n",
    "def close(self):\n",
    "        \"\"\"Close the Snowflake connection if open.\"\"\"\n",
    "        if self._conn:\n",
    "            self._conn.close()\n",
    "            self._conn = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec5cff0",
   "metadata": {},
   "source": [
    "Awesome, now we can connect to Snowflake and close the connection when we are done with it. Let's also add a method to execute a query and return the results. This will be useful for getting data from Snowflake. We can do this by adding an `execute_query` method to our class. This method will take a query string as an argument and return the results of the query. We can use the `cursor` object to execute the query and fetch the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dc6ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch_to(SnowflakeConnector, cls_method=True)\n",
    "def execute_query(self, query: str, params: Optional[List[Any]] = None) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Execute a query and return results as a list of dictionaries.\n",
    "    \n",
    "    Args:\n",
    "        query: SQL query to execute\n",
    "        params: Optional parameters for the query\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries with query results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = self._get_connection()\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Execute the query\n",
    "        if params:\n",
    "            cursor.execute(query, params)\n",
    "        else:\n",
    "            cursor.execute(query)\n",
    "        \n",
    "        # Get column names\n",
    "        columns = [desc[0] for desc in cursor.description]\n",
    "        \n",
    "        # Convert results to list of dictionaries\n",
    "        results = []\n",
    "        for row in cursor:\n",
    "            results.append(dict(zip(columns, row)))\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error executing query: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd296693",
   "metadata": {},
   "source": [
    "## Data Insertion\n",
    "\n",
    "Let's also create a way to insert data into Snowflake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa70879",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@patch_to(SnowflakeConnector, cls_method=True)\n",
    "def _get_table_schema(self, table_name: str, use_cache: bool = True) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Query Snowflake to get the schema of a table.\n",
    "    \n",
    "    Args:\n",
    "        table_name: Name of the table\n",
    "        use_cache: Whether to use cached schema information\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping column names to their data types\n",
    "    \"\"\"\n",
    "    # Use instance values as defaults if not provided\n",
    "    database = self.database\n",
    "    schema = self.schema\n",
    "    \n",
    "    if not database or not schema:\n",
    "        raise ValueError(\"Database and schema must be provided\")\n",
    "    \n",
    "    # Check cache first\n",
    "    cache_key = f\"{database}.{schema}.{table_name}\"\n",
    "    if use_cache and cache_key in self._schema_cache:\n",
    "        return self._schema_cache[cache_key]\n",
    "        \n",
    "    schema_query = f\"\"\"\n",
    "        SELECT \n",
    "            COLUMN_NAME, \n",
    "            DATA_TYPE \n",
    "        FROM \n",
    "            {database}.INFORMATION_SCHEMA.COLUMNS \n",
    "        WHERE \n",
    "            TABLE_SCHEMA = '{schema}' \n",
    "            AND TABLE_NAME = '{table_name}'\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        conn = self._get_connection()\n",
    "        with conn.cursor() as cs:\n",
    "            cs.execute(schema_query)\n",
    "            results = cs.fetchall()\n",
    "            result_schema = {row[0]: row[1] for row in results}\n",
    "            \n",
    "            # Cache the result\n",
    "            if use_cache:\n",
    "                self._schema_cache[cache_key] = result_schema\n",
    "                \n",
    "            return result_schema\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting schema: {e}\")\n",
    "        raise RuntimeError(f\"Failed to get schema for {table_name}: {str(e)}\")\n",
    "\n",
    "\n",
    "@patch_to(SnowflakeConnector, cls_method=True)\n",
    "def _get_current_timestamp(self, timezone: str = 'America/Chicago') -> str:\n",
    "    \"\"\"Get current timestamp in the specified timezone.\"\"\"\n",
    "    tz = pytz.timezone(timezone)\n",
    "    return datetime.now(tz).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "@patch_to(SnowflakeConnector, cls_method=True)\n",
    "def _create_schema_mapping(self, db_schema: Dict) -> Dict[str, str]:\n",
    "    \"\"\"Create a case-insensitive mapping of schema columns.\"\"\"\n",
    "    return {k.upper(): k for k in db_schema.keys()}\n",
    "\n",
    "\n",
    "@patch_to(SnowflakeConnector, cls_method=True)\n",
    "def _prepare_insert_data(\n",
    "        self,\n",
    "        insert_data: Dict, \n",
    "        db_schema: Dict,\n",
    "        schema_keys_map: Dict[str, str],\n",
    "        auto_timestamp: bool = True,\n",
    "        include_all_columns: bool = False,\n",
    "        timezone: str = 'America/Chicago',\n",
    "        debug: bool = False\n",
    "    ) -> Dict:\n",
    "    \"\"\"\n",
    "    Prepare and filter data for insertion based on schema.\n",
    "    \"\"\"\n",
    "    filtered_data = {}\n",
    "    current_time = self._get_current_timestamp(timezone)\n",
    "    \n",
    "    if include_all_columns:\n",
    "        for schema_col in db_schema.keys():\n",
    "            filtered_data[schema_col] = None\n",
    "    \n",
    "    for k, v in insert_data.items():\n",
    "        k_upper = k.upper()\n",
    "        if k_upper in schema_keys_map:\n",
    "            # Use the actual case from the schema\n",
    "            original_case_key = schema_keys_map[k_upper]\n",
    "            filtered_data[original_case_key] = v\n",
    "    \n",
    "    if auto_timestamp:\n",
    "        timestamp_fields = {\n",
    "            'UPDATED_AT': 'updated_at',\n",
    "            'CREATED_AT': 'created_at',\n",
    "            'RUN_TIME': 'run_time',\n",
    "            'INSERTED_AT': 'inserted_at',\n",
    "            'TIMESTAMP': 'timestamp'\n",
    "        }\n",
    "        \n",
    "        for upper_field, lower_field in timestamp_fields.items():\n",
    "            if upper_field in schema_keys_map:\n",
    "                original_case = schema_keys_map[upper_field]\n",
    "                \n",
    "                field_provided = False\n",
    "                for k in insert_data.keys():\n",
    "                    if k.upper() == upper_field:\n",
    "                        field_provided = True\n",
    "                        break\n",
    "                \n",
    "                if not field_provided:\n",
    "                    filtered_data[original_case] = current_time\n",
    "    \n",
    "    if debug and not filtered_data:\n",
    "        print(f\"Available columns in schema: {list(db_schema.keys())}\")\n",
    "        print(f\"Provided keys: {list(insert_data.keys())}\")\n",
    "        print(f\"Case-insensitive mapping: {schema_keys_map}\")\n",
    "        \n",
    "    return filtered_data\n",
    "\n",
    "\n",
    "@patch_to(SnowflakeConnector, cls_method=True)\n",
    "def _build_insert_query(self, table_name: str, columns: List[str]) -> str:\n",
    "    \"\"\"Build the base INSERT query for Snowflake.\"\"\"\n",
    "    return f\"\"\"\n",
    "        INSERT INTO {self.database}.{self.schema}.{table_name} (\n",
    "            {', '.join(columns)}\n",
    "        )\n",
    "        SELECT\n",
    "    \"\"\"\n",
    "\n",
    "@patch_to(SnowflakeConnector, cls_method=True)\n",
    "def _prepare_values_and_expressions(\n",
    "    self,\n",
    "    filtered_data: Dict, \n",
    "    columns: List[str], \n",
    "    db_schema: Dict\n",
    "    ) -> Tuple[List, List[str]]:\n",
    "    \"\"\"Prepare values and SQL expressions for the query.\"\"\"\n",
    "    values = []\n",
    "    select_exprs = []\n",
    "    \n",
    "    for col in columns:\n",
    "        value = filtered_data[col]\n",
    "        data_type = db_schema[col].upper()\n",
    "        \n",
    "        # Handle nulls consistently\n",
    "        if value is None:\n",
    "            select_exprs.append(\"NULL\")\n",
    "            continue\n",
    "            \n",
    "        if any(array_type in data_type for array_type in ['ARRAY', 'LIST']):\n",
    "            if isinstance(value, list) and len(value) == 0:\n",
    "                select_exprs.append(\"ARRAY_CONSTRUCT()\")\n",
    "            elif isinstance(value, list):\n",
    "                placeholders = \", \".join([\"%s\"] * len(value))\n",
    "                select_exprs.append(f\"ARRAY_CONSTRUCT({placeholders})\")\n",
    "                values.extend(value)\n",
    "            else:\n",
    "                # Handle single value as array\n",
    "                select_exprs.append(\"ARRAY_CONSTRUCT(%s)\")\n",
    "                values.append(value)\n",
    "        \n",
    "        elif any(obj_type in data_type for obj_type in ['OBJECT', 'VARIANT', 'JSON']):\n",
    "            # Handle JSON object with PARSE_JSON - convert to string first\n",
    "            select_exprs.append(\"PARSE_JSON(%s)\")\n",
    "            # Always convert to JSON string, even if it's already a dict\n",
    "            if isinstance(value, (dict, list)):\n",
    "                values.append(json.dumps(value))\n",
    "            elif isinstance(value, str):\n",
    "                # If it's already a string, validate it's valid JSON\n",
    "                try:\n",
    "                    json.loads(value)  # Validate JSON\n",
    "                    values.append(value)\n",
    "                except json.JSONDecodeError:\n",
    "                    # If not valid JSON, wrap it as a string value\n",
    "                    values.append(json.dumps(value))\n",
    "            else:\n",
    "                # For other types, convert to JSON\n",
    "                values.append(json.dumps(value))\n",
    "        \n",
    "        else:\n",
    "            select_exprs.append(\"%s\")\n",
    "            values.append(value)\n",
    "    \n",
    "    return values, select_exprs\n",
    "\n",
    "# Additional helper method to validate JSON data before insertion\n",
    "@patch_to(SnowflakeConnector, cls_method=True)\n",
    "def _validate_json_data(self, data: Dict[str, Any], db_schema: Dict) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Validate and convert JSON data types before insertion.\n",
    "    \n",
    "    Args:\n",
    "        data: Data dictionary to validate\n",
    "        db_schema: Table schema dictionary\n",
    "        \n",
    "    Returns:\n",
    "        Validated data dictionary with properly formatted JSON strings\n",
    "    \"\"\"\n",
    "    validated_data = {}\n",
    "    \n",
    "    for key, value in data.items():\n",
    "        if key in db_schema:\n",
    "            data_type = db_schema[key].upper()\n",
    "            \n",
    "            # Handle JSON/VARIANT/OBJECT types\n",
    "            if any(obj_type in data_type for obj_type in ['OBJECT', 'VARIANT', 'JSON']):\n",
    "                if value is None:\n",
    "                    validated_data[key] = None\n",
    "                elif isinstance(value, str):\n",
    "                    # Validate existing JSON string\n",
    "                    try:\n",
    "                        json.loads(value)\n",
    "                        validated_data[key] = value\n",
    "                    except json.JSONDecodeError:\n",
    "                        # Convert invalid JSON string to valid JSON\n",
    "                        validated_data[key] = json.dumps(value)\n",
    "                else:\n",
    "                    # Convert dict, list, or other types to JSON string\n",
    "                    validated_data[key] = json.dumps(value)\n",
    "            else:\n",
    "                validated_data[key] = value\n",
    "        else:\n",
    "            validated_data[key] = value\n",
    "    \n",
    "    return validated_data\n",
    "\n",
    "@patch_to(SnowflakeConnector, cls_method=True)\n",
    "def _execute_insert_query(\n",
    "    self,\n",
    "    database: str,\n",
    "    schema: str,\n",
    "    query: str,\n",
    "    values: List,\n",
    "    debug: bool = False\n",
    "    ) -> bool:\n",
    "    \"\"\"Execute the INSERT query on Snowflake.\"\"\"\n",
    "    try:\n",
    "        if debug:\n",
    "            print(\"DEBUG SQL:\", query)\n",
    "            print(\"DEBUG values:\", values)\n",
    "            print(\"DEBUG values count:\", len(values))\n",
    "            \n",
    "        conn = self._get_connection()\n",
    "        with conn.cursor() as cs:\n",
    "            cs.execute(query, values)\n",
    "            conn.commit()\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error inserting data: {e}\")\n",
    "        if debug:\n",
    "            print(f\"Failed SQL: {query}\")\n",
    "            print(f\"Values: {values}\")\n",
    "        raise RuntimeError(f\"Failed to insert data: {str(e)}\")\n",
    "\n",
    "\n",
    "@patch_to(SnowflakeConnector, cls_method=True)\n",
    "def insert_record(self, table_name: str, data: Dict[str, Any], **kwargs) -> Any:\n",
    "    \"\"\"\n",
    "    Enhanced function to insert data into Snowflake with improved type handling.\n",
    "    \n",
    "    Args:\n",
    "        table_name: Name of the table to insert into\n",
    "        data: Dictionary of data to insert\n",
    "        **kwargs: Additional options for insertion\n",
    "        \n",
    "    Returns:\n",
    "        ID of the inserted record (if id_field is provided) or True if successful\n",
    "    \"\"\"\n",
    "    # Set default options\n",
    "    options = {\n",
    "        'database': self.database,\n",
    "        'schema': self.schema,\n",
    "        'auto_timestamp': True,\n",
    "        'timezone': 'America/Chicago',\n",
    "        'debug': False,\n",
    "        'include_all_columns': False\n",
    "    }\n",
    "    \n",
    "    # Update with any provided options\n",
    "    options.update(kwargs)\n",
    "    \n",
    "    if not options['database'] or not options['schema']:\n",
    "        raise ValueError(\"Database and schema must be provided\")\n",
    "    \n",
    "    # Get table schema\n",
    "    db_schema = self._get_table_schema(table_name)\n",
    "    print('db_schema', db_schema, 'sc')\n",
    "    if not db_schema:\n",
    "        raise ValueError(f\"Could not retrieve schema for {options['database']}.{options['schema']}.{table_name}\")\n",
    "        \n",
    "    if options['debug']:\n",
    "        print(f\"Table schema: {list(db_schema.keys())}\")\n",
    "        \n",
    "    # Create case-insensitive mapping\n",
    "    schema_keys_map = self._create_schema_mapping(db_schema)\n",
    "    \n",
    "    # Prepare and filter data\n",
    "    filtered_data = self._prepare_insert_data(\n",
    "        data,\n",
    "        db_schema,\n",
    "        schema_keys_map,\n",
    "        options['auto_timestamp'],\n",
    "        options['include_all_columns'],\n",
    "        options['timezone'],\n",
    "        options['debug']\n",
    "    )\n",
    "    \n",
    "    # Nothing to insert\n",
    "    if not filtered_data:\n",
    "        print(\"Warning: No valid columns provided for insert\")\n",
    "        return False\n",
    "    \n",
    "    # Build the query\n",
    "    columns = list(filtered_data.keys())\n",
    "    insert_query = self._build_insert_query(table_name,columns)\n",
    "    \n",
    "    # Prepare values and expressions\n",
    "    values, select_exprs = self._prepare_values_and_expressions(\n",
    "        filtered_data, \n",
    "        columns, \n",
    "        db_schema\n",
    "    )\n",
    "    \n",
    "    # Complete the query\n",
    "    insert_query += \", \".join(select_exprs)\n",
    "    \n",
    "    self._execute_insert_query(\n",
    "        options['database'],\n",
    "        options['schema'],\n",
    "        insert_query,\n",
    "        values,\n",
    "        options['debug']\n",
    "    )\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db0ecbb",
   "metadata": {},
   "source": [
    "Now that we can insert individual rows, let's also add a method to insert multiple rows at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e083e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@patch_to(SnowflakeConnector, cls_method=True)\n",
    "def bulk_insert(self, table_name: str, df: pd.DataFrame, **kwargs) -> bool:\n",
    "    \"\"\"\n",
    "    Enhanced function to bulk insert DataFrame data into Snowflake with improved type handling.\n",
    "    \n",
    "    Args:\n",
    "        table_name: Name of the table to insert into\n",
    "        df: DataFrame with columns matching table structure\n",
    "        **kwargs: Additional options for insertion\n",
    "            - database: Override default database\n",
    "            - schema: Override default schema\n",
    "            - auto_timestamp: Add timestamps to standard timestamp fields (default: True)\n",
    "            - timezone: Timezone for timestamps (default: 'America/Chicago')\n",
    "            - debug: Enable debug output (default: False)\n",
    "            - batch_size: Rows per batch (default: 10000)\n",
    "            - quote_identifiers: Whether to quote identifiers (default: False)\n",
    "            - chunk_size: Size of DataFrame chunks to process (default: None)\n",
    "            - auto_create_table: Create table if not exists (default: False)\n",
    "            - use_column_mapping: Map DataFrame columns to table columns (default: True)\n",
    "        \n",
    "    Returns:\n",
    "        True if successful\n",
    "    \"\"\"\n",
    "    # Set default options\n",
    "    options = {\n",
    "        'database': self.database,\n",
    "        'schema': self.schema,\n",
    "        'auto_timestamp': True,\n",
    "        'timezone': 'America/Chicago',\n",
    "        'debug': False,\n",
    "        'batch_size': 10000,\n",
    "        'quote_identifiers': False,\n",
    "        'chunk_size': None,\n",
    "        'auto_create_table': False,\n",
    "        'use_column_mapping': True\n",
    "    }\n",
    "    \n",
    "    # Update with any provided options\n",
    "    options.update(kwargs)\n",
    "    \n",
    "    if not options['database'] or not options['schema']:\n",
    "        raise ValueError(\"Database and schema must be provided\")\n",
    "\n",
    "    # Get table schema\n",
    "    try:\n",
    "        db_schema = self._get_table_schema(table_name)\n",
    "    except Exception as e:\n",
    "        if options['debug']:\n",
    "            print(f\"Error getting schema: {e}\")\n",
    "        if options['auto_create_table']:\n",
    "            # Auto-create table logic would go here\n",
    "            # Not implemented in this version\n",
    "            raise NotImplementedError(\"Auto-create table not implemented yet\")\n",
    "        else:\n",
    "            raise ValueError(f\"Could not retrieve schema for {options['database']}.{options['schema']}.{table_name}\")\n",
    "\n",
    "    if options['debug']:\n",
    "        print(f\"Table schema: {list(db_schema.keys())}\")\n",
    "        print(f\"DataFrame columns: {list(df.columns)}\")\n",
    "\n",
    "    # Create case-insensitive mapping\n",
    "    schema_keys_map = self._create_schema_mapping(db_schema)\n",
    "    \n",
    "    # Process DataFrame to match schema if needed\n",
    "    if options['use_column_mapping']:\n",
    "        df_processed = self._prepare_dataframe(\n",
    "            df,\n",
    "            db_schema,\n",
    "            schema_keys_map,\n",
    "            options['auto_timestamp'],\n",
    "            options['timezone'],\n",
    "            options['debug']\n",
    "        )\n",
    "    else:\n",
    "        df_processed = df.copy()\n",
    "    \n",
    "    if df_processed.empty:\n",
    "        print(\"Warning: No valid data to insert after processing\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        conn = self._get_connection()\n",
    "        \n",
    "        # Set database and schema context\n",
    "        qualified_table = f\"{options['database']}.{options['schema']}.{table_name}\"\n",
    "        \n",
    "        if options['debug']:\n",
    "            print(f\"Inserting into: {qualified_table}\")\n",
    "            print(f\"Processed DataFrame shape: {df_processed.shape}\")\n",
    "        \n",
    "        # Use the Snowflake Pandas integration\n",
    "        success, num_chunks, num_rows, output = write_pandas(\n",
    "            conn=conn,\n",
    "            df=df_processed,\n",
    "            table_name=qualified_table,\n",
    "            quote_identifiers=options['quote_identifiers'],\n",
    "            chunk_size=options['chunk_size'],\n",
    "            compression='gzip',  # Usually a good default\n",
    "            parallel=4,          # Use parallel processing\n",
    "            overwrite=False,     # Append mode\n",
    "            auto_create_table=False  # We handle schema validation separately\n",
    "        )\n",
    "        \n",
    "        if success:\n",
    "            #print(f\"Bulk insert into {qualified_table}: {num_rows} rows in {num_chunks} chunks\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Bulk insert failed: {output}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        if options['debug']:\n",
    "            print(f\"Error bulk inserting records: {e}\")\n",
    "            print(f\"DataFrame columns: {list(df_processed.columns)}\")\n",
    "            print(f\"First row: {df_processed.iloc[0].to_dict() if not df_processed.empty else 'Empty DataFrame'}\")\n",
    "            \n",
    "        raise RuntimeError(f\"Failed to bulk insert data: {str(e)}\")\n",
    "\n",
    "\n",
    "@patch_to(SnowflakeConnector, cls_method=True)\n",
    "def _prepare_dataframe(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        db_schema: Dict,\n",
    "        schema_keys_map: Dict[str, str],\n",
    "        auto_timestamp: bool = True,\n",
    "        timezone: str = 'America/Chicago',\n",
    "        debug: bool = False\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepare DataFrame for insertion based on table schema.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        db_schema: Table schema dictionary\n",
    "        schema_keys_map: Case-insensitive column mapping\n",
    "        auto_timestamp: Whether to add timestamps automatically\n",
    "        timezone: Timezone for timestamps\n",
    "        debug: Enable debug output\n",
    "        \n",
    "    Returns:\n",
    "        Processed DataFrame ready for insertion\n",
    "    \"\"\"\n",
    "    # Create a new DataFrame with correctly cased columns\n",
    "    result_df = pd.DataFrame()\n",
    "    current_time = self._get_current_timestamp(timezone)\n",
    "    \n",
    "    # Add all schema columns that exist in input DataFrame\n",
    "    for col in db_schema.keys():\n",
    "        col_upper = col.upper()\n",
    "        \n",
    "        # Check if column exists in input DataFrame (case-insensitive)\n",
    "        matching_cols = [c for c in df.columns if c.upper() == col_upper]\n",
    "        \n",
    "        if matching_cols:\n",
    "            # Use the first matching column from input\n",
    "            input_col = matching_cols[0]\n",
    "            result_df[col] = df[input_col].copy()\n",
    "            \n",
    "            # Handle special types based on schema\n",
    "            data_type = db_schema[col].upper()\n",
    "            \n",
    "            # Convert array types\n",
    "            if any(array_type in data_type for array_type in ['ARRAY', 'LIST']):\n",
    "                # Ensure values are properly formatted as lists\n",
    "                result_df[col] = result_df[col].apply(\n",
    "                    lambda x: x if (isinstance(x, list) or x is None) else [x]\n",
    "                )\n",
    "                \n",
    "            # Convert object/JSON types - FIX: Convert to JSON strings properly\n",
    "            elif any(obj_type in data_type for obj_type in ['OBJECT', 'VARIANT', 'JSON']):\n",
    "                # Convert values to JSON strings for DataFrame bulk insert\n",
    "                def convert_to_json_string(x):\n",
    "                    if x is None:\n",
    "                        return None\n",
    "                    elif isinstance(x, str):\n",
    "                        # If it's already a string, validate and return as-is\n",
    "                        try:\n",
    "                            json.loads(x)  # Validate JSON\n",
    "                            return x\n",
    "                        except json.JSONDecodeError:\n",
    "                            # If not valid JSON, wrap it as a JSON string\n",
    "                            return json.dumps(x)\n",
    "                    else:\n",
    "                        # Convert dict, list, or other types to JSON string\n",
    "                        return json.dumps(x)\n",
    "                \n",
    "                result_df[col] = result_df[col].apply(convert_to_json_string)\n",
    "    \n",
    "    # Add timestamp columns if requested\n",
    "    if auto_timestamp:\n",
    "        timestamp_fields = {\n",
    "            'UPDATED_AT': 'updated_at',\n",
    "            'CREATED_AT': 'created_at',\n",
    "            'RUN_TIME': 'run_time',\n",
    "            'INSERTED_AT': 'inserted_at',\n",
    "            'TIMESTAMP': 'timestamp'\n",
    "        }\n",
    "        \n",
    "        for upper_field, lower_field in timestamp_fields.items():\n",
    "            if upper_field in schema_keys_map:\n",
    "                original_case = schema_keys_map[upper_field]\n",
    "                \n",
    "                # Skip if column already exists in result DataFrame\n",
    "                if original_case in result_df.columns:\n",
    "                    continue\n",
    "                    \n",
    "                # Add timestamp column\n",
    "                result_df[original_case] = current_time\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Original columns: {df.columns.tolist()}\")\n",
    "        print(f\"Processed columns: {result_df.columns.tolist()}\")\n",
    "        print(f\"Schema columns: {list(db_schema.keys())}\")\n",
    "        \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e2c8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@patch_to(SnowflakeConnector, cls_method=True)    \n",
    "def get_user_interactions(self, user_id: str, limit: int = 100) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Get recent interactions for a specific user.\n",
    "    \n",
    "    Args:\n",
    "        user_id: Slack user ID\n",
    "        limit: Maximum number of records to return\n",
    "        \n",
    "    Returns:\n",
    "        List of interaction records\n",
    "    \"\"\"\n",
    "    query = f\"\"\"\n",
    "        SELECT * \n",
    "        FROM SLACK_INTERACTIONS \n",
    "        WHERE USER_ID = %s \n",
    "        ORDER BY TIMESTAMP DESC \n",
    "        LIMIT {limit}\n",
    "    \"\"\"\n",
    "    return self.execute_query(query, [user_id])\n",
    "\n",
    "@patch_to(SnowflakeConnector, cls_method=True)\n",
    "def get_interactions_by_view(self, view: str, limit: int = 100) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Get recent interactions for a specific view.\n",
    "    \n",
    "    Args:\n",
    "        view: View name\n",
    "        limit: Maximum number of records to return\n",
    "        \n",
    "    Returns:\n",
    "        List of interaction records\n",
    "    \"\"\"\n",
    "    query = f\"\"\"\n",
    "        SELECT * \n",
    "        FROM SLACK_INTERACTIONS \n",
    "        WHERE VIEW = %s \n",
    "        ORDER BY TIMESTAMP DESC \n",
    "        LIMIT {limit}\n",
    "    \"\"\"\n",
    "    return self.execute_query(query, [view])\n",
    "\n",
    "@patch_to(SnowflakeConnector, cls_method=True)\n",
    "def get_interaction_summary(self) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Get a summary of interactions by view and action type.\n",
    "    \n",
    "    Returns:\n",
    "        Summary statistics\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "        SELECT \n",
    "            VIEW,\n",
    "            ACTION_TYPE,\n",
    "            COUNT(*) AS INTERACTION_COUNT,\n",
    "            COUNT(DISTINCT USER_ID) AS UNIQUE_USERS,\n",
    "            MIN(TIMESTAMP) AS FIRST_INTERACTION,\n",
    "            MAX(TIMESTAMP) AS LATEST_INTERACTION\n",
    "        FROM SLACK_INTERACTIONS\n",
    "        GROUP BY VIEW, ACTION_TYPE\n",
    "        ORDER BY VIEW, INTERACTION_COUNT DESC\n",
    "    \"\"\"\n",
    "    return self.execute_query(query)\n",
    "\n",
    "def __del__(self):\n",
    "    \"\"\"Ensure connection is closed when object is destroyed.\"\"\"\n",
    "    self.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291542f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
